{"id":"index.html","dependencies":[{"name":"./src\\src\\css\\styles.css","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\src\\css\\styles.css","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\src\\css\\MovingButtons.css","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\src\\css\\MovingButtons.css","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\css\\style.css","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\css\\style.css","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\js\\three.min.js","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\js\\three.min.js","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\src\\js\\stats.min.js","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\src\\js\\stats.min.js","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\src\\js\\ColladaLoader.js","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\src\\js\\ColladaLoader.js","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\src\\js\\FlyControls.js","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\src\\js\\FlyControls.js","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\js\\convnet.js","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\js\\convnet.js","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\js\\util.js","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\js\\util.js","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\js\\vis.js","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\js\\vis.js","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\js\\tfjs-brain.js","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\js\\tfjs-brain.js","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"./src\\js\\rldemo.js","dynamic":true,"resolved":"D:\\workspace\\q_deep_learn_tfjs\\src\\js\\rldemo.js","parent":"D:\\workspace\\q_deep_learn_tfjs\\index.html"},{"name":"D:\\workspace\\q_deep_learn_tfjs\\.babelrc","includedInParent":true,"mtime":1587756406752},{"name":"D:\\workspace\\q_deep_learn_tfjs\\package.json","includedInParent":true,"mtime":1587808184875}],"generated":{"html":"<!doctype html>\r\n<html lang=\"en\">\r\n <head>\r\n  <meta charset=\"utf-8\">\r\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome=1\">\r\n  <title>ConvNetJS Deep Q Learning Reinforcement Learning with Neural Network demo</title>\r\n  <meta name=\"description\" content=\"\">\r\n  <meta name=\"author\" content=\"\">\r\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\r\n  <link rel=\"stylesheet\" type=\"text/css\" href=\"1ab0f83f35e4737261fb6d83ffd75fd5.css\">\r\n  <link rel=\"stylesheet\" type=\"text/css\" href=\"a5a760c0b418107f2357f7a8646efc22.css\">\r\n  <link rel=\"stylesheet\" href=\"df3a09ffcba83a8d0209ba07967ba849.css\">\r\n  <script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs\"> </script>\r\n<script src=\"https://cdn.jsdelivr.net/npm/@tensorflow/tfjs-vis\"></script>\r\n  <script src=\"b49dbb010417c7e6e7d52c0642e6395e.js\"></script>\r\n  <script src=\"7347323264ab960acd0a7c14fd5d8f96.js\"></script>\r\n  <script src=\"10bcf8f9dc0e11e1011676e2a1cc8da2.js\"></script>\r\n  <script src=\"67f206ebe018210822866fcc4b46551e.js\"></script>\r\n  <script src=\"9b5a3a30753f153dc36fa8caa5654eaf.js\"></script>\r\n  <script src=\"cfd20667517a40535d42e774608aa97b.js\"></script>\r\n  <script src=\"b3348ee0015c5eecfdc9469c3fa66810.js\"></script>\r\n  <!-- <script src=\"./src/js/deepqlearn.js\"></script> -->\r\n  <script src=\"d0a3aaf3da2a02ab6ad1c03e6acf33c7.js\"></script>\r\n  \r\n  <script src=\"6f41b94355d4fb7f7727db4461708d69.js\"></script>\r\n  <style>canvas { border: 1px solid white; }</style>\r\n\r\n </head>\r\n <body onload=\"start();\">\r\n   <div id=\"wrap\">\r\n   <h2><a href=\"http://cs.stanford.edu/people/karpathy/convnetjs/\">ConvNetJS</a> Deep Q Learning Demo</h2>\r\n   <h1>Description</h1>\r\n   <p>\r\n   This demo follows the description of the Deep Q Learning algorithm described in \r\n   <a href=\"http://arxiv.org/pdf/1312.5602v1.pdf\">Playing Atari with Deep Reinforcement Learning</a>, \r\n   a paper from NIPS 2013 Deep Learning Workshop from DeepMind. The paper is a nice demo of a fairly\r\n   standard (model-free) Reinforcement Learning algorithm (Q Learning) learning to play Atari games.\r\n   </p>\r\n   <p>\r\n   In this demo, instead of Atari games, we'll start out with something more simple: \r\n   a 2D agent that has 9 eyes pointing in different angles ahead and every eye senses 3 values\r\n   along its direction (up to a certain maximum visibility distance): distance to a wall, distance to \r\n   a green thing, or distance to a red thing. The agent navigates by using one of 5 actions that turn \r\n   it different angles. The red things are apples and the agent gets reward for eating them. The green\r\n   things are poison and the agent gets negative reward for eating them. The training takes a few tens\r\n   of minutes with current parameter settings.\r\n   </p>\r\n   <p>\r\n   Over time, the agent learns to avoid states that lead to states with low rewards, and picks actions\r\n   that lead to better states instead.\r\n   </p>\r\n   <h1>Q-Learner full specification and options</h1>\r\n   <p>\r\n   The textfield below gets eval()'d to produce the Q-learner for this demo. This allows you to fiddle with \r\n   various parameters and settings and also shows how you can use the API for your own purposes. \r\n   All of these settings are optional but are listed to give an idea of possibilities.\r\n   Feel free to change things around and hit reload! Documentation for all\r\n   options is the paper linked to above, and there are also \r\n   comments for every option in the source code javascript file.\r\n   </p>\r\n   <textarea id=\"qspec\" style=\"width:100%; height:200px;\">\r\nvar num_inputs = 27; // 9 eyes, each sees 3 numbers (wall, green, red thing proximity)\r\nvar num_actions = 5; // 5 possible angles agent can turn\r\nvar temporal_window = 1; // amount of temporal memory. 0 = agent lives in-the-moment :)\r\nvar network_size = num_inputs*temporal_window + num_actions*temporal_window + num_inputs;\r\n\r\n// the value function network computes a value of taking any of the possible actions\r\n// given an input state. Here we specify one explicitly the hard way\r\n// but user could also equivalently instead use opt.hidden_layer_sizes = [20,20]\r\n// to just insert simple relu hidden layers.\r\nvar layer_defs = [];\r\nlayer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:network_size});\r\nlayer_defs.push({type:'fc', num_neurons: 50, activation:'relu'});\r\nlayer_defs.push({type:'fc', num_neurons: 50, activation:'relu'});\r\nlayer_defs.push({type:'regression', num_neurons:num_actions});\r\n\r\n// options for the Temporal Difference learner that trains the above net\r\n// by backpropping the temporal difference learning rule.\r\nvar tdtrainer_options = {learning_rate:0.001, momentum:0.0, batch_size:64, l2_decay:0.01};\r\n\r\nvar opt = {};\r\nopt.temporal_window = temporal_window;\r\nopt.experience_size = 30000;\r\nopt.start_learn_threshold = 1000;\r\nopt.gamma = 0.7;\r\nopt.learning_steps_total = 200000;\r\nopt.learning_steps_burnin = 3000;\r\nopt.epsilon_min = 0.05;\r\nopt.epsilon_test_time = 0.05;\r\nopt.layer_defs = layer_defs;\r\nopt.tdtrainer_options = tdtrainer_options;\r\n\r\nvar brain = new deepqlearn.Brain(num_inputs, num_actions, opt); // woohoo\r\n   </textarea>\r\n   <button onclick=\"reload()\" style=\"width: 200px; height: 30px; margin-top: 5px;\">Reload</button>\r\n   \r\n   <h1>State Visualizations</h1>\r\n   \r\n   <div><b>Left</b>: Current input state (quite a useless thing to look at). <b>Right</b>: Average reward over time (this should go up as agent becomes better on average at collecting rewards)</div>\r\n   <canvas id=\"vis_canvas\" width=\"350\" height=\"150\"></canvas>\r\n   <canvas id=\"graph_canvas\" width=\"350\" height=\"150\"></canvas><br>\r\n   <canvas id=\"net_canvas\" width=\"700\" height=\"200\"></canvas><br>\r\n\r\n   <div style=\"font-size:13px;\">\r\n   (Takes ~10 minutes to train with current settings. If you're impatient, scroll down and load an example pre-trained network from pre-filled JSON)\r\n   </div>\r\n   <canvas id=\"canvas\" width=\"700\" height=\"500\"></canvas>\r\n   <div id=\"brain_info_div\"></div>\r\n   \r\n   <h1>Controls</h1>\r\n   <button onclick=\"goveryfast()\">Go very fast</button>\r\n   <button onclick=\"gofast()\">Go fast</button>\r\n   <button onclick=\"gonormal()\">Go normal speed</button>\r\n   <button onclick=\"goslow()\">Go slow</button><br>\r\n   <button onclick=\"startlearn()\">Start Learning</button>\r\n   <button onclick=\"stoplearn()\">Stop Learning</button>\r\n   \r\n   <h1>I/O</h1>\r\n   <p>\r\n   You can save and load a network from JSON here. Note that the textfield is prefilled with a\r\n   pretrained network that works reasonable well, if you're impatient to let yours train enough.\r\n   Just hit the load button!\r\n   </p>\r\n   <button onclick=\"savenet()\">Save network to JSON</button>\r\n   <button onclick=\"loadnet()\">Load network from JSON</button>\r\n \r\n   <br>\r\n   \r\n   </div>\r\n </body>\r\n</html>\r\n"},"hash":"65fd656d4fad340299f3e3b264094b74","cacheData":{"env":{}}}