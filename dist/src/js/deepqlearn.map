{"version":3,"sources":["src/js/deepqlearn.js"],"names":["Experience","state0","action0","reward0","state1","Brain","num_states","num_actions","opt","temporal_window","experience_size","start_learn_threshold","Math","floor","min","gamma","learning_steps_total","learning_steps_burnin","epsilon_min","epsilon_test_time","random_action_distribution","length","console","log","a","s","k","abs","net_inputs","window_size","max","state_window","Array","action_window","reward_window","net_window","layer_defs","type","out_depth","out_sx","out_sy","num_neurons","push","hidden_layer_sizes","hl","activation","value_net","convnetjs","Net","makeLayers","tdtrainer_options","learning_rate","momentum","batch_size","l2_decay","tdtrainer","SGDTrainer","experience","age","forward_passes","epsilon","latest_reward","last_input_array","average_reward_window","cnnutil","Window","average_loss_window","learning","prototype","random_action","randi","p","randf","cumprob","policy","svol","Vol","w","action_values","forward","maxk","maxval","action","value","getNetInput","xt","concat","n","action1ofk","q","input_array","net_input","rf","maxact","shift","backward","reward","add","e","ri","avcost","re","x","r","ystruct","dim","val","loss","train","get_average","visSelf","elt","innerHTML","brainvis","document","createElement","desc","t","appendChild"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAE,cAEA;AACA;AACA;AACA;;AACA,IAAIA,UAAU,GAAG,UAASC,MAAT,EAAiBC,OAAjB,EAA0BC,OAA1B,EAAmCC,MAAnC,EAA2C;AAC1D,OAAKH,MAAL,GAAcA,MAAd;AACA,OAAKC,OAAL,GAAeA,OAAf;AACA,OAAKC,OAAL,GAAeA,OAAf;AACA,OAAKC,MAAL,GAAcA,MAAd;AACD,CALD,EAOA;AACA;AACA;;;AACA,IAAIC,KAAK,GAAG,UAASC,UAAT,EAAqBC,WAArB,EAAkCC,GAAlC,EAAuC;AACjD,MAAIA,GAAG,GAAGA,GAAG,IAAI,EAAjB,CADiD,CAEjD;AACA;AACA;;AACA,OAAKC,eAAL,GAAuB,OAAOD,GAAG,CAACC,eAAX,KAA+B,WAA/B,GAA6CD,GAAG,CAACC,eAAjD,GAAmE,CAA1F,CALiD,CAMjD;;AACA,OAAKC,eAAL,GAAuB,OAAOF,GAAG,CAACE,eAAX,KAA+B,WAA/B,GAA6CF,GAAG,CAACE,eAAjD,GAAmE,KAA1F,CAPiD,CAQjD;;AACA,OAAKC,qBAAL,GAA6B,OAAOH,GAAG,CAACG,qBAAX,KAAqC,WAArC,GAAkDH,GAAG,CAACG,qBAAtD,GAA8EC,IAAI,CAACC,KAAL,CAAWD,IAAI,CAACE,GAAL,CAAS,KAAKJ,eAAL,GAAqB,GAA9B,EAAmC,IAAnC,CAAX,CAA3G,CATiD,CAUjD;;AACA,OAAKK,KAAL,GAAa,OAAOP,GAAG,CAACO,KAAX,KAAqB,WAArB,GAAmCP,GAAG,CAACO,KAAvC,GAA+C,GAA5D,CAXiD,CAajD;;AACA,OAAKC,oBAAL,GAA4B,OAAOR,GAAG,CAACQ,oBAAX,KAAoC,WAApC,GAAkDR,GAAG,CAACQ,oBAAtD,GAA6E,MAAzG,CAdiD,CAejD;;AACA,OAAKC,qBAAL,GAA6B,OAAOT,GAAG,CAACS,qBAAX,KAAqC,WAArC,GAAmDT,GAAG,CAACS,qBAAvD,GAA+E,IAA5G,CAhBiD,CAiBjD;;AACA,OAAKC,WAAL,GAAmB,OAAOV,GAAG,CAACU,WAAX,KAA2B,WAA3B,GAAyCV,GAAG,CAACU,WAA7C,GAA2D,IAA9E,CAlBiD,CAmBjD;;AACA,OAAKC,iBAAL,GAAyB,OAAOX,GAAG,CAACW,iBAAX,KAAiC,WAAjC,GAA+CX,GAAG,CAACW,iBAAnD,GAAuE,IAAhG,CApBiD,CAsBjD;AACA;;AACA,MAAG,OAAOX,GAAG,CAACY,0BAAX,KAA0C,WAA7C,EAA0D;AACxD;AACA,SAAKA,0BAAL,GAAkCZ,GAAG,CAACY,0BAAtC;;AACA,QAAG,KAAKA,0BAAL,CAAgCC,MAAhC,KAA2Cd,WAA9C,EAA2D;AACzDe,MAAAA,OAAO,CAACC,GAAR,CAAY,2EAAZ;AACD;;AACD,QAAIC,CAAC,GAAG,KAAKJ,0BAAb;AACA,QAAIK,CAAC,GAAG,GAAR;;AAAa,SAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACF,CAAC,CAACH,MAAhB,EAAuBK,CAAC,EAAxB,EAA4B;AAAED,MAAAA,CAAC,IAAGD,CAAC,CAACE,CAAD,CAAL;AAAW;;AACtD,QAAGd,IAAI,CAACe,GAAL,CAASF,CAAC,GAAC,GAAX,IAAgB,MAAnB,EAA2B;AAAEH,MAAAA,OAAO,CAACC,GAAR,CAAY,sDAAZ;AAAsE;AACpG,GATD,MASO;AACL,SAAKH,0BAAL,GAAkC,EAAlC;AACD,GAnCgD,CAqCjD;AACA;AACA;AACA;;;AACA,OAAKQ,UAAL,GAAkBtB,UAAU,GAAG,KAAKG,eAAlB,GAAoCF,WAAW,GAAG,KAAKE,eAAvD,GAAyEH,UAA3F;AACA,OAAKA,UAAL,GAAkBA,UAAlB;AACA,OAAKC,WAAL,GAAmBA,WAAnB;AACA,OAAKsB,WAAL,GAAmBjB,IAAI,CAACkB,GAAL,CAAS,KAAKrB,eAAd,EAA+B,CAA/B,CAAnB,CA5CiD,CA4CK;;AACtD,OAAKsB,YAAL,GAAoB,IAAIC,KAAJ,CAAU,KAAKH,WAAf,CAApB;AACA,OAAKI,aAAL,GAAqB,IAAID,KAAJ,CAAU,KAAKH,WAAf,CAArB;AACA,OAAKK,aAAL,GAAqB,IAAIF,KAAJ,CAAU,KAAKH,WAAf,CAArB;AACA,OAAKM,UAAL,GAAkB,IAAIH,KAAJ,CAAU,KAAKH,WAAf,CAAlB,CAhDiD,CAkDjD;;AACA,MAAIO,UAAU,GAAG,EAAjB;;AACA,MAAG,OAAO5B,GAAG,CAAC4B,UAAX,KAA0B,WAA7B,EAA0C;AACxC;AACA;AACA;AACAA,IAAAA,UAAU,GAAG5B,GAAG,CAAC4B,UAAjB;;AACA,QAAGA,UAAU,CAACf,MAAX,GAAoB,CAAvB,EAA0B;AAAEC,MAAAA,OAAO,CAACC,GAAR,CAAY,sCAAZ;AAAsD;;AAClF,QAAGa,UAAU,CAAC,CAAD,CAAV,CAAcC,IAAd,KAAuB,OAA1B,EAAmC;AAAEf,MAAAA,OAAO,CAACC,GAAR,CAAY,2CAAZ;AAA2D;;AAChG,QAAGa,UAAU,CAACA,UAAU,CAACf,MAAX,GAAkB,CAAnB,CAAV,CAAgCgB,IAAhC,KAAyC,YAA5C,EAA0D;AAAEf,MAAAA,OAAO,CAACC,GAAR,CAAY,+CAAZ;AAA+D;;AAC3H,QAAGa,UAAU,CAAC,CAAD,CAAV,CAAcE,SAAd,GAA0BF,UAAU,CAAC,CAAD,CAAV,CAAcG,MAAxC,GAAiDH,UAAU,CAAC,CAAD,CAAV,CAAcI,MAA/D,KAA0E,KAAKZ,UAAlF,EAA8F;AAC5FN,MAAAA,OAAO,CAACC,GAAR,CAAY,8GAAZ;AACD;;AACD,QAAGa,UAAU,CAACA,UAAU,CAACf,MAAX,GAAkB,CAAnB,CAAV,CAAgCoB,WAAhC,KAAgD,KAAKlC,WAAxD,EAAqE;AACnEe,MAAAA,OAAO,CAACC,GAAR,CAAY,8DAAZ;AACD;AACF,GAdD,MAcO;AACL;AACAa,IAAAA,UAAU,CAACM,IAAX,CAAgB;AAACL,MAAAA,IAAI,EAAC,OAAN;AAAeE,MAAAA,MAAM,EAAC,CAAtB;AAAyBC,MAAAA,MAAM,EAAC,CAAhC;AAAmCF,MAAAA,SAAS,EAAC,KAAKV;AAAlD,KAAhB;;AACA,QAAG,OAAOpB,GAAG,CAACmC,kBAAX,KAAkC,WAArC,EAAkD;AAChD;AACA,UAAIC,EAAE,GAAGpC,GAAG,CAACmC,kBAAb;;AACA,WAAI,IAAIjB,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACkB,EAAE,CAACvB,MAAjB,EAAwBK,CAAC,EAAzB,EAA6B;AAC3BU,QAAAA,UAAU,CAACM,IAAX,CAAgB;AAACL,UAAAA,IAAI,EAAC,IAAN;AAAYI,UAAAA,WAAW,EAACG,EAAE,CAAClB,CAAD,CAA1B;AAA+BmB,UAAAA,UAAU,EAAC;AAA1C,SAAhB,EAD2B,CACyC;AACrE;AACF;;AACDT,IAAAA,UAAU,CAACM,IAAX,CAAgB;AAACL,MAAAA,IAAI,EAAC,YAAN;AAAoBI,MAAAA,WAAW,EAAClC;AAAhC,KAAhB,EAVK,CAU0D;AAChE;;AACD,OAAKuC,SAAL,GAAiB,IAAIC,SAAS,CAACC,GAAd,EAAjB;AACA,OAAKF,SAAL,CAAeG,UAAf,CAA0Bb,UAA1B,EA/EiD,CAiFjD;;AACA,MAAIc,iBAAiB,GAAG;AAACC,IAAAA,aAAa,EAAC,IAAf;AAAqBC,IAAAA,QAAQ,EAAC,GAA9B;AAAmCC,IAAAA,UAAU,EAAC,EAA9C;AAAkDC,IAAAA,QAAQ,EAAC;AAA3D,GAAxB;;AACA,MAAG,OAAO9C,GAAG,CAAC0C,iBAAX,KAAiC,WAApC,EAAiD;AAC/CA,IAAAA,iBAAiB,GAAG1C,GAAG,CAAC0C,iBAAxB,CAD+C,CACJ;AAC5C;;AACD,OAAKK,SAAL,GAAiB,IAAIR,SAAS,CAACS,UAAd,CAAyB,KAAKV,SAA9B,EAAyCI,iBAAzC,CAAjB,CAtFiD,CAwFjD;;AACA,OAAKO,UAAL,GAAkB,EAAlB,CAzFiD,CA2FjD;;AACA,OAAKC,GAAL,GAAW,CAAX,CA5FiD,CA4FnC;;AACd,OAAKC,cAAL,GAAsB,CAAtB,CA7FiD,CA6FxB;;AACzB,OAAKC,OAAL,GAAe,GAAf,CA9FiD,CA8F7B;;AACpB,OAAKC,aAAL,GAAqB,CAArB;AACA,OAAKC,gBAAL,GAAwB,EAAxB;AACA,OAAKC,qBAAL,GAA6B,IAAIC,OAAO,CAACC,MAAZ,CAAmB,IAAnB,EAAyB,EAAzB,CAA7B;AACA,OAAKC,mBAAL,GAA2B,IAAIF,OAAO,CAACC,MAAZ,CAAmB,IAAnB,EAAyB,EAAzB,CAA3B;AACA,OAAKE,QAAL,GAAgB,IAAhB;AACD,CApGD;;AAqGA9D,KAAK,CAAC+D,SAAN,GAAkB;AAChBC,EAAAA,aAAa,EAAE,YAAW;AACxB;AACA;AACA;AACA;AACA,QAAG,KAAKjD,0BAAL,CAAgCC,MAAhC,KAA2C,CAA9C,EAAiD;AAC/C,aAAO0B,SAAS,CAACuB,KAAV,CAAgB,CAAhB,EAAmB,KAAK/D,WAAxB,CAAP;AACD,KAFD,MAEO;AACL;AACA,UAAIgE,CAAC,GAAGxB,SAAS,CAACyB,KAAV,CAAgB,CAAhB,EAAmB,GAAnB,CAAR;AACA,UAAIC,OAAO,GAAG,GAAd;;AACA,WAAI,IAAI/C,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKnB,WAAnB,EAA+BmB,CAAC,EAAhC,EAAoC;AAClC+C,QAAAA,OAAO,IAAI,KAAKrD,0BAAL,CAAgCM,CAAhC,CAAX;;AACA,YAAG6C,CAAC,GAAGE,OAAP,EAAgB;AAAE,iBAAO/C,CAAP;AAAW;AAC9B;AACF;AACF,GAjBe;AAkBhBgD,EAAAA,MAAM,EAAE,UAASjD,CAAT,EAAY;AAClB;AACA;AACA,QAAIkD,IAAI,GAAG,IAAI5B,SAAS,CAAC6B,GAAd,CAAkB,CAAlB,EAAqB,CAArB,EAAwB,KAAKhD,UAA7B,CAAX;AACA+C,IAAAA,IAAI,CAACE,CAAL,GAASpD,CAAT;AACA,QAAIqD,aAAa,GAAG,KAAKhC,SAAL,CAAeiC,OAAf,CAAuBJ,IAAvB,CAApB;AACA,QAAIK,IAAI,GAAG,CAAX;AACA,QAAIC,MAAM,GAAGH,aAAa,CAACD,CAAd,CAAgB,CAAhB,CAAb;;AACA,SAAI,IAAInD,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKnB,WAAnB,EAA+BmB,CAAC,EAAhC,EAAoC;AAClC,UAAGoD,aAAa,CAACD,CAAd,CAAgBnD,CAAhB,IAAqBuD,MAAxB,EAAgC;AAAED,QAAAA,IAAI,GAAGtD,CAAP;AAAUuD,QAAAA,MAAM,GAAGH,aAAa,CAACD,CAAd,CAAgBnD,CAAhB,CAAT;AAA8B;AAC3E;;AACD,WAAO;AAACwD,MAAAA,MAAM,EAACF,IAAR;AAAcG,MAAAA,KAAK,EAACF;AAApB,KAAP;AACD,GA9Be;AA+BhBG,EAAAA,WAAW,EAAE,UAASC,EAAT,EAAa;AACxB;AACA;AACA,QAAIR,CAAC,GAAG,EAAR;AACAA,IAAAA,CAAC,GAAGA,CAAC,CAACS,MAAF,CAASD,EAAT,CAAJ,CAJwB,CAIN;AAClB;;AACA,QAAIE,CAAC,GAAG,KAAK1D,WAAb;;AACA,SAAI,IAAIH,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKjB,eAAnB,EAAmCiB,CAAC,EAApC,EAAwC;AACtC;AACAmD,MAAAA,CAAC,GAAGA,CAAC,CAACS,MAAF,CAAS,KAAKvD,YAAL,CAAkBwD,CAAC,GAAC,CAAF,GAAI7D,CAAtB,CAAT,CAAJ,CAFsC,CAGtC;AACA;;AACA,UAAI8D,UAAU,GAAG,IAAIxD,KAAJ,CAAU,KAAKzB,WAAf,CAAjB;;AACA,WAAI,IAAIkF,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKlF,WAAnB,EAA+BkF,CAAC,EAAhC,EAAoCD,UAAU,CAACC,CAAD,CAAV,GAAgB,GAAhB;;AACpCD,MAAAA,UAAU,CAAC,KAAKvD,aAAL,CAAmBsD,CAAC,GAAC,CAAF,GAAI7D,CAAvB,CAAD,CAAV,GAAwC,MAAI,KAAKpB,UAAjD;AACAuE,MAAAA,CAAC,GAAGA,CAAC,CAACS,MAAF,CAASE,UAAT,CAAJ;AACD;;AACD,WAAOX,CAAP;AACD,GAjDe;AAkDhBE,EAAAA,OAAO,EAAE,UAASW,WAAT,EAAsB;AAC7B;AACA,SAAK/B,cAAL,IAAuB,CAAvB;AACA,SAAKG,gBAAL,GAAwB4B,WAAxB,CAH6B,CAGQ;AAErC;;AACA,QAAIR,MAAJ;;AACA,QAAG,KAAKvB,cAAL,GAAsB,KAAKlD,eAA9B,EAA+C;AAC7C;AACA,UAAIkF,SAAS,GAAG,KAAKP,WAAL,CAAiBM,WAAjB,CAAhB;;AACA,UAAG,KAAKvB,QAAR,EAAkB;AAChB;AACA,aAAKP,OAAL,GAAehD,IAAI,CAACE,GAAL,CAAS,GAAT,EAAcF,IAAI,CAACkB,GAAL,CAAS,KAAKZ,WAAd,EAA2B,MAAI,CAAC,KAAKwC,GAAL,GAAW,KAAKzC,qBAAjB,KAAyC,KAAKD,oBAAL,GAA4B,KAAKC,qBAA1E,CAA/B,CAAd,CAAf;AACD,OAHD,MAGO;AACL,aAAK2C,OAAL,GAAe,KAAKzC,iBAApB,CADK,CACkC;AACxC;;AACD,UAAIyE,EAAE,GAAG7C,SAAS,CAACyB,KAAV,CAAgB,CAAhB,EAAkB,CAAlB,CAAT;;AACA,UAAGoB,EAAE,GAAG,KAAKhC,OAAb,EAAsB;AACpB;AACAsB,QAAAA,MAAM,GAAG,KAAKb,aAAL,EAAT;AACD,OAHD,MAGO;AACL;AACA,YAAIwB,MAAM,GAAG,KAAKnB,MAAL,CAAYiB,SAAZ,CAAb;AACAT,QAAAA,MAAM,GAAGW,MAAM,CAACX,MAAhB;AACF;AACD,KAlBD,MAkBO;AACL;AACA;AACA,UAAIS,SAAS,GAAG,EAAhB;AACAT,MAAAA,MAAM,GAAG,KAAKb,aAAL,EAAT;AACD,KA9B4B,CAgC7B;;;AACA,SAAKlC,UAAL,CAAgB2D,KAAhB;AACA,SAAK3D,UAAL,CAAgBO,IAAhB,CAAqBiD,SAArB;AACA,SAAK5D,YAAL,CAAkB+D,KAAlB;AACA,SAAK/D,YAAL,CAAkBW,IAAlB,CAAuBgD,WAAvB;AACA,SAAKzD,aAAL,CAAmB6D,KAAnB;AACA,SAAK7D,aAAL,CAAmBS,IAAnB,CAAwBwC,MAAxB;AAEA,WAAOA,MAAP;AACD,GA3Fe;AA4FhBa,EAAAA,QAAQ,EAAE,UAASC,MAAT,EAAiB;AACzB,SAAKnC,aAAL,GAAqBmC,MAArB;AACA,SAAKjC,qBAAL,CAA2BkC,GAA3B,CAA+BD,MAA/B;AACA,SAAK9D,aAAL,CAAmB4D,KAAnB;AACA,SAAK5D,aAAL,CAAmBQ,IAAnB,CAAwBsD,MAAxB;;AAEA,QAAG,CAAC,KAAK7B,QAAT,EAAmB;AAAE;AAAS,KANL,CAQzB;;;AACA,SAAKT,GAAL,IAAY,CAAZ,CATyB,CAWzB;AACA;;AACA,QAAG,KAAKC,cAAL,GAAsB,KAAKlD,eAAL,GAAuB,CAAhD,EAAmD;AACjD,UAAIyF,CAAC,GAAG,IAAIlG,UAAJ,EAAR;AACA,UAAIuF,CAAC,GAAG,KAAK1D,WAAb;AACAqE,MAAAA,CAAC,CAACjG,MAAF,GAAW,KAAKkC,UAAL,CAAgBoD,CAAC,GAAC,CAAlB,CAAX;AACAW,MAAAA,CAAC,CAAChG,OAAF,GAAY,KAAK+B,aAAL,CAAmBsD,CAAC,GAAC,CAArB,CAAZ;AACAW,MAAAA,CAAC,CAAC/F,OAAF,GAAY,KAAK+B,aAAL,CAAmBqD,CAAC,GAAC,CAArB,CAAZ;AACAW,MAAAA,CAAC,CAAC9F,MAAF,GAAW,KAAK+B,UAAL,CAAgBoD,CAAC,GAAC,CAAlB,CAAX;;AACA,UAAG,KAAK9B,UAAL,CAAgBpC,MAAhB,GAAyB,KAAKX,eAAjC,EAAkD;AAChD,aAAK+C,UAAL,CAAgBf,IAAhB,CAAqBwD,CAArB;AACD,OAFD,MAEO;AACL;AACA,YAAIC,EAAE,GAAGpD,SAAS,CAACuB,KAAV,CAAgB,CAAhB,EAAmB,KAAK5D,eAAxB,CAAT;AACA,aAAK+C,UAAL,CAAgB0C,EAAhB,IAAsBD,CAAtB;AACD;AACF,KA3BwB,CA6BzB;AACA;;;AACA,QAAG,KAAKzC,UAAL,CAAgBpC,MAAhB,GAAyB,KAAKV,qBAAjC,EAAwD;AACtD,UAAIyF,MAAM,GAAG,GAAb;;AACA,WAAI,IAAI1E,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAG,KAAK6B,SAAL,CAAeF,UAA/B,EAA0C3B,CAAC,EAA3C,EAA+C;AAC7C,YAAI2E,EAAE,GAAGtD,SAAS,CAACuB,KAAV,CAAgB,CAAhB,EAAmB,KAAKb,UAAL,CAAgBpC,MAAnC,CAAT;AACA,YAAI6E,CAAC,GAAG,KAAKzC,UAAL,CAAgB4C,EAAhB,CAAR;AACA,YAAIC,CAAC,GAAG,IAAIvD,SAAS,CAAC6B,GAAd,CAAkB,CAAlB,EAAqB,CAArB,EAAwB,KAAKhD,UAA7B,CAAR;AACA0E,QAAAA,CAAC,CAACzB,CAAF,GAAMqB,CAAC,CAACjG,MAAR;AACA,YAAI4F,MAAM,GAAG,KAAKnB,MAAL,CAAYwB,CAAC,CAAC9F,MAAd,CAAb;AACA,YAAImG,CAAC,GAAGL,CAAC,CAAC/F,OAAF,GAAY,KAAKY,KAAL,GAAa8E,MAAM,CAACV,KAAxC;AACA,YAAIqB,OAAO,GAAG;AAACC,UAAAA,GAAG,EAAEP,CAAC,CAAChG,OAAR;AAAiBwG,UAAAA,GAAG,EAAEH;AAAtB,SAAd;AACA,YAAII,IAAI,GAAG,KAAKpD,SAAL,CAAeqD,KAAf,CAAqBN,CAArB,EAAwBE,OAAxB,CAAX;AACAJ,QAAAA,MAAM,IAAIO,IAAI,CAACA,IAAf;AACD;;AACDP,MAAAA,MAAM,GAAGA,MAAM,GAAC,KAAK7C,SAAL,CAAeF,UAA/B;AACA,WAAKa,mBAAL,CAAyB+B,GAAzB,CAA6BG,MAA7B;AACA9E,MAAAA,OAAO,CAACC,GAAR,CAAY,KAAK2C,mBAAL,CAAyB2C,WAAzB,EAAZ;AACD;AACF,GA5Ie;AA6IhBC,EAAAA,OAAO,EAAE,UAASC,GAAT,EAAc;AACrBA,IAAAA,GAAG,CAACC,SAAJ,GAAgB,EAAhB,CADqB,CACD;AAEpB;;AACA,QAAIC,QAAQ,GAAGC,QAAQ,CAACC,aAAT,CAAuB,KAAvB,CAAf,CAJqB,CAMrB;;AACA,QAAIC,IAAI,GAAGF,QAAQ,CAACC,aAAT,CAAuB,KAAvB,CAAX;AACA,QAAIE,CAAC,GAAG,EAAR;AACAA,IAAAA,CAAC,IAAI,6BAA6B,KAAK5D,UAAL,CAAgBpC,MAA7C,GAAsD,MAA3D;AACAgG,IAAAA,CAAC,IAAI,0BAA0B,KAAKzD,OAA/B,GAAyC,MAA9C;AACAyD,IAAAA,CAAC,IAAI,UAAU,KAAK3D,GAAf,GAAqB,MAA1B;AACA2D,IAAAA,CAAC,IAAI,8BAA8B,KAAKnD,mBAAL,CAAyB2C,WAAzB,EAA9B,GAAuE,QAA5E;AACAQ,IAAAA,CAAC,IAAI,wBAAwB,KAAKtD,qBAAL,CAA2B8C,WAA3B,EAAxB,GAAmE,QAAxE;AACAO,IAAAA,IAAI,CAACJ,SAAL,GAAiBK,CAAjB;AACAJ,IAAAA,QAAQ,CAACK,WAAT,CAAqBF,IAArB;AAEAL,IAAAA,GAAG,CAACO,WAAJ,CAAgBL,QAAhB;AACD;AA/Je,CAAlB","file":"deepqlearn.map","sourceRoot":"..","sourcesContent":["  \"use strict\";\r\n  \r\n  // An agent is in state0 and does action0\r\n  // environment then assigns reward0 and provides new state, state1\r\n  // Experience nodes store all this information, which is used in the\r\n  // Q-learning update step\r\n  var Experience = function(state0, action0, reward0, state1) {\r\n    this.state0 = state0;\r\n    this.action0 = action0;\r\n    this.reward0 = reward0;\r\n    this.state1 = state1;\r\n  }\r\n\r\n  // A Brain object does all the magic.\r\n  // over time it receives some inputs and some rewards\r\n  // and its job is to set the outputs to maximize the expected reward\r\n  var Brain = function(num_states, num_actions, opt) {\r\n    var opt = opt || {};\r\n    // in number of time steps, of temporal memory\r\n    // the ACTUAL input to the net will be (x,a) temporal_window times, and followed by current x\r\n    // so to have no information from previous time step going into value function, set to 0.\r\n    this.temporal_window = typeof opt.temporal_window !== 'undefined' ? opt.temporal_window : 1; \r\n    // size of experience replay memory\r\n    this.experience_size = typeof opt.experience_size !== 'undefined' ? opt.experience_size : 30000;\r\n    // number of examples in experience replay memory before we begin learning\r\n    this.start_learn_threshold = typeof opt.start_learn_threshold !== 'undefined'? opt.start_learn_threshold : Math.floor(Math.min(this.experience_size*0.1, 1000)); \r\n    // gamma is a crucial parameter that controls how much plan-ahead the agent does. In [0,1]\r\n    this.gamma = typeof opt.gamma !== 'undefined' ? opt.gamma : 0.8;\r\n    \r\n    // number of steps we will learn for\r\n    this.learning_steps_total = typeof opt.learning_steps_total !== 'undefined' ? opt.learning_steps_total : 100000;\r\n    // how many steps of the above to perform only random actions (in the beginning)?\r\n    this.learning_steps_burnin = typeof opt.learning_steps_burnin !== 'undefined' ? opt.learning_steps_burnin : 3000;\r\n    // what epsilon value do we bottom out on? 0.0 => purely deterministic policy at end\r\n    this.epsilon_min = typeof opt.epsilon_min !== 'undefined' ? opt.epsilon_min : 0.05;\r\n    // what epsilon to use at test time? (i.e. when learning is disabled)\r\n    this.epsilon_test_time = typeof opt.epsilon_test_time !== 'undefined' ? opt.epsilon_test_time : 0.01;\r\n    \r\n    // advanced feature. Sometimes a random action should be biased towards some values\r\n    // for example in flappy bird, we may want to choose to not flap more often\r\n    if(typeof opt.random_action_distribution !== 'undefined') {\r\n      // this better sum to 1 by the way, and be of length this.num_actions\r\n      this.random_action_distribution = opt.random_action_distribution;\r\n      if(this.random_action_distribution.length !== num_actions) {\r\n        console.log('TROUBLE. random_action_distribution should be same length as num_actions.');\r\n      }\r\n      var a = this.random_action_distribution;\r\n      var s = 0.0; for(var k=0;k<a.length;k++) { s+= a[k]; }\r\n      if(Math.abs(s-1.0)>0.0001) { console.log('TROUBLE. random_action_distribution should sum to 1!'); }\r\n    } else {\r\n      this.random_action_distribution = [];\r\n    }\r\n    \r\n    // states that go into neural net to predict optimal action look as\r\n    // x0,a0,x1,a1,x2,a2,...xt\r\n    // this variable controls the size of that temporal window. Actions are\r\n    // encoded as 1-of-k hot vectors\r\n    this.net_inputs = num_states * this.temporal_window + num_actions * this.temporal_window + num_states;\r\n    this.num_states = num_states;\r\n    this.num_actions = num_actions;\r\n    this.window_size = Math.max(this.temporal_window, 2); // must be at least 2, but if we want more context even more\r\n    this.state_window = new Array(this.window_size);\r\n    this.action_window = new Array(this.window_size);\r\n    this.reward_window = new Array(this.window_size);\r\n    this.net_window = new Array(this.window_size);\r\n    \r\n    // create [state -> value of all possible actions] modeling net for the value function\r\n    var layer_defs = [];\r\n    if(typeof opt.layer_defs !== 'undefined') {\r\n      // this is an advanced usage feature, because size of the input to the network, and number of\r\n      // actions must check out. This is not very pretty Object Oriented programming but I can't see\r\n      // a way out of it :(\r\n      layer_defs = opt.layer_defs;\r\n      if(layer_defs.length < 2) { console.log('TROUBLE! must have at least 2 layers'); }\r\n      if(layer_defs[0].type !== 'input') { console.log('TROUBLE! first layer must be input layer!'); }\r\n      if(layer_defs[layer_defs.length-1].type !== 'regression') { console.log('TROUBLE! last layer must be input regression!'); }\r\n      if(layer_defs[0].out_depth * layer_defs[0].out_sx * layer_defs[0].out_sy !== this.net_inputs) {\r\n        console.log('TROUBLE! Number of inputs must be num_states * temporal_window + num_actions * temporal_window + num_states!');\r\n      }\r\n      if(layer_defs[layer_defs.length-1].num_neurons !== this.num_actions) {\r\n        console.log('TROUBLE! Number of regression neurons should be num_actions!');\r\n      }\r\n    } else {\r\n      // create a very simple neural net by default\r\n      layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:this.net_inputs});\r\n      if(typeof opt.hidden_layer_sizes !== 'undefined') {\r\n        // allow user to specify this via the option, for convenience\r\n        var hl = opt.hidden_layer_sizes;\r\n        for(var k=0;k<hl.length;k++) {\r\n          layer_defs.push({type:'fc', num_neurons:hl[k], activation:'relu'}); // relu by default\r\n        }\r\n      }\r\n      layer_defs.push({type:'regression', num_neurons:num_actions}); // value function output\r\n    }\r\n    this.value_net = new convnetjs.Net();\r\n    this.value_net.makeLayers(layer_defs);\r\n    \r\n    // and finally we need a Temporal Difference Learning trainer!\r\n    var tdtrainer_options = {learning_rate:0.01, momentum:0.0, batch_size:64, l2_decay:0.01};\r\n    if(typeof opt.tdtrainer_options !== 'undefined') {\r\n      tdtrainer_options = opt.tdtrainer_options; // allow user to overwrite this\r\n    }\r\n    this.tdtrainer = new convnetjs.SGDTrainer(this.value_net, tdtrainer_options);\r\n    \r\n    // experience replay\r\n    this.experience = [];\r\n    \r\n    // various housekeeping variables\r\n    this.age = 0; // incremented every backward()\r\n    this.forward_passes = 0; // incremented every forward()\r\n    this.epsilon = 1.0; // controls exploration exploitation tradeoff. Should be annealed over time\r\n    this.latest_reward = 0;\r\n    this.last_input_array = [];\r\n    this.average_reward_window = new cnnutil.Window(1000, 10);\r\n    this.average_loss_window = new cnnutil.Window(1000, 10);\r\n    this.learning = true;\r\n  }\r\n  Brain.prototype = {\r\n    random_action: function() {\r\n      // a bit of a helper function. It returns a random action\r\n      // we are abstracting this away because in future we may want to \r\n      // do more sophisticated things. For example some actions could be more\r\n      // or less likely at \"rest\"/default state.\r\n      if(this.random_action_distribution.length === 0) {\r\n        return convnetjs.randi(0, this.num_actions);\r\n      } else {\r\n        // okay, lets do some fancier sampling:\r\n        var p = convnetjs.randf(0, 1.0);\r\n        var cumprob = 0.0;\r\n        for(var k=0;k<this.num_actions;k++) {\r\n          cumprob += this.random_action_distribution[k];\r\n          if(p < cumprob) { return k; }\r\n        }\r\n      }\r\n    },\r\n    policy: function(s) {\r\n      // compute the value of doing any action in this state\r\n      // and return the argmax action and its value\r\n      var svol = new convnetjs.Vol(1, 1, this.net_inputs);\r\n      svol.w = s;\r\n      var action_values = this.value_net.forward(svol);\r\n      var maxk = 0; \r\n      var maxval = action_values.w[0];\r\n      for(var k=1;k<this.num_actions;k++) {\r\n        if(action_values.w[k] > maxval) { maxk = k; maxval = action_values.w[k]; }\r\n      }\r\n      return {action:maxk, value:maxval};\r\n    },\r\n    getNetInput: function(xt) {\r\n      // return s = (x,a,x,a,x,a,xt) state vector. \r\n      // It's a concatenation of last window_size (x,a) pairs and current state x\r\n      var w = [];\r\n      w = w.concat(xt); // start with current state\r\n      // and now go backwards and append states and actions from history temporal_window times\r\n      var n = this.window_size; \r\n      for(var k=0;k<this.temporal_window;k++) {\r\n        // state\r\n        w = w.concat(this.state_window[n-1-k]);\r\n        // action, encoded as 1-of-k indicator vector. We scale it up a bit because\r\n        // we dont want weight regularization to undervalue this information, as it only exists once\r\n        var action1ofk = new Array(this.num_actions);\r\n        for(var q=0;q<this.num_actions;q++) action1ofk[q] = 0.0;\r\n        action1ofk[this.action_window[n-1-k]] = 1.0*this.num_states;\r\n        w = w.concat(action1ofk);\r\n      }\r\n      return w;\r\n    },\r\n    forward: function(input_array) {\r\n      // compute forward (behavior) pass given the input neuron signals from body\r\n      this.forward_passes += 1;\r\n      this.last_input_array = input_array; // back this up\r\n      \r\n      // create network input\r\n      var action;\r\n      if(this.forward_passes > this.temporal_window) {\r\n        // we have enough to actually do something reasonable\r\n        var net_input = this.getNetInput(input_array);\r\n        if(this.learning) {\r\n          // compute epsilon for the epsilon-greedy policy\r\n          this.epsilon = Math.min(1.0, Math.max(this.epsilon_min, 1.0-(this.age - this.learning_steps_burnin)/(this.learning_steps_total - this.learning_steps_burnin))); \r\n        } else {\r\n          this.epsilon = this.epsilon_test_time; // use test-time value\r\n        }\r\n        var rf = convnetjs.randf(0,1);\r\n        if(rf < this.epsilon) {\r\n          // choose a random action with epsilon probability\r\n          action = this.random_action();\r\n        } else {\r\n          // otherwise use our policy to make decision\r\n          var maxact = this.policy(net_input);\r\n          action = maxact.action;\r\n       }\r\n      } else {\r\n        // pathological case that happens first few iterations \r\n        // before we accumulate window_size inputs\r\n        var net_input = [];\r\n        action = this.random_action();\r\n      }\r\n      \r\n      // remember the state and action we took for backward pass\r\n      this.net_window.shift();\r\n      this.net_window.push(net_input);\r\n      this.state_window.shift(); \r\n      this.state_window.push(input_array);\r\n      this.action_window.shift(); \r\n      this.action_window.push(action);\r\n      \r\n      return action;\r\n    },\r\n    backward: function(reward) {\r\n      this.latest_reward = reward;\r\n      this.average_reward_window.add(reward);\r\n      this.reward_window.shift();\r\n      this.reward_window.push(reward);\r\n      \r\n      if(!this.learning) { return; } \r\n      \r\n      // various book-keeping\r\n      this.age += 1;\r\n      \r\n      // it is time t+1 and we have to store (s_t, a_t, r_t, s_{t+1}) as new experience\r\n      // (given that an appropriate number of state measurements already exist, of course)\r\n      if(this.forward_passes > this.temporal_window + 1) {\r\n        var e = new Experience();\r\n        var n = this.window_size;\r\n        e.state0 = this.net_window[n-2];\r\n        e.action0 = this.action_window[n-2];\r\n        e.reward0 = this.reward_window[n-2];\r\n        e.state1 = this.net_window[n-1];\r\n        if(this.experience.length < this.experience_size) {\r\n          this.experience.push(e);\r\n        } else {\r\n          // replace. finite memory!\r\n          var ri = convnetjs.randi(0, this.experience_size);\r\n          this.experience[ri] = e;\r\n        }\r\n      }\r\n      \r\n      // learn based on experience, once we have some samples to go on\r\n      // this is where the magic happens...\r\n      if(this.experience.length > this.start_learn_threshold) {\r\n        var avcost = 0.0;\r\n        for(var k=0;k < this.tdtrainer.batch_size;k++) {\r\n          var re = convnetjs.randi(0, this.experience.length);\r\n          var e = this.experience[re];\r\n          var x = new convnetjs.Vol(1, 1, this.net_inputs);\r\n          x.w = e.state0;\r\n          var maxact = this.policy(e.state1);\r\n          var r = e.reward0 + this.gamma * maxact.value;\r\n          var ystruct = {dim: e.action0, val: r};\r\n          var loss = this.tdtrainer.train(x, ystruct);\r\n          avcost += loss.loss;\r\n        }\r\n        avcost = avcost/this.tdtrainer.batch_size;\r\n        this.average_loss_window.add(avcost);\r\n        console.log(this.average_loss_window.get_average())\r\n      }\r\n    },\r\n    visSelf: function(elt) {\r\n      elt.innerHTML = ''; // erase elt first\r\n      \r\n      // elt is a DOM element that this function fills with brain-related information\r\n      var brainvis = document.createElement('div');\r\n      \r\n      // basic information\r\n      var desc = document.createElement('div');\r\n      var t = '';\r\n      t += 'experience replay size: ' + this.experience.length + '<br>';\r\n      t += 'exploration epsilon: ' + this.epsilon + '<br>';\r\n      t += 'age: ' + this.age + '<br>';\r\n      t += 'average Q-learning loss: ' + this.average_loss_window.get_average() + '<br />';\r\n      t += 'smooth-ish reward: ' + this.average_reward_window.get_average() + '<br />';\r\n      desc.innerHTML = t;\r\n      brainvis.appendChild(desc);\r\n      \r\n      elt.appendChild(brainvis);\r\n    }\r\n  }\r\n\r\n"]}