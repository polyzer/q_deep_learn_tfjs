{"version":3,"sources":["src/js/tfjs-brain.js"],"names":["Experience","constructor","state0","action0","reward0","state1","Brain","num_states","num_actions","opt","temporal_window","experience_size","start_learn_threshold","Math","floor","min","gamma","learning_steps_total","learning_steps_burnin","epsilon_min","epsilon_test_time","vis","tfvis","visor","surface","name","tab","drawArea","random_action_distribution","length","console","log","a","s","k","abs","net_inputs","window_size","max","state_window","Array","action_window","reward_window","net_window","layer_defs","NN","tf","sequential","add","layers","dense","inputShape","units","activation","kernelInitializer","kernelRegularizer","BATCH_SIZE","optimizer","train","sgd","experience","age","forward_passes","epsilon","latest_reward","last_input_array","average_reward_window","cnnutil","Window","average_loss_window","learning","random_action","convnetjs","randi","p","randf","cumprob","policy","svol","Vol","w","tens","tensor","reshape","action_values","apply","ret","action","dataSync","value","argMax","dispose","getNetInput","xt","concat","n","action1ofk","q","forward","input_array","net_input","rf","random","maxact","shift","push","sampleExperience","batchSize","trainOnReplayBatch","batch","replayMemory","sample","lossFunction","tidy","stateTensor","getStateTensor","map","example","game","height","width","actionTensor","tensor1d","qs","onlineNetwork","training","mul","oneHot","NUM_ACTIONS","sum","rewardTensor","nextStateTensor","nextMaxQTensor","targetNetwork","predict","doneMask","scalar","sub","asType","targetQs","losses","meanSquaredError","grads","variableGrads","applyGradients","backward","reward","e","ri","x_tensor","x","y_tensor","y","y_s","y_new","output","shape","loss","mulStrict","square","avcost","re","r","getWeights","show","layer","getLayer","print","get_average","visSelf","elt","innerHTML","brainvis","document","createElement","desc","t","appendChild"],"mappings":";;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAAE,cAEA;AACA;AACA;AACA;;AACA,MAAMA,UAAN,CAAkB;AAChBC,EAAAA,WAAW,CAACC,MAAD,EAASC,OAAT,EAAkBC,OAAlB,EAA2BC,MAA3B,EAAkC;AAC3C,SAAKH,MAAL,GAAcA,MAAd;AACA,SAAKC,OAAL,GAAeA,OAAf;AACA,SAAKC,OAAL,GAAeA,OAAf;AACA,SAAKC,MAAL,GAAcA,MAAd;AACD;;AANe,EASlB;AACA;AACA;;;AACA,MAAMC,KAAN,CAAY;AACVL,EAAAA,WAAW,CAACM,UAAD,EAAaC,WAAb,EAA0BC,GAAG,GAAC,EAA9B,EAAkC;AAC3C;AACA;AACA;AACA,SAAKC,eAAL,GAAuB,OAAOD,GAAG,CAACC,eAAX,KAA+B,WAA/B,GAA6CD,GAAG,CAACC,eAAjD,GAAmE,CAA1F,CAJ2C,CAK3C;;AACA,SAAKC,eAAL,GAAuB,OAAOF,GAAG,CAACE,eAAX,KAA+B,WAA/B,GAA6CF,GAAG,CAACE,eAAjD,GAAmE,KAA1F,CAN2C,CAO3C;;AACA,SAAKC,qBAAL,GAA6B,OAAOH,GAAG,CAACG,qBAAX,KAAqC,WAArC,GAAkDH,GAAG,CAACG,qBAAtD,GAA8EC,IAAI,CAACC,KAAL,CAAWD,IAAI,CAACE,GAAL,CAAS,KAAKJ,eAAL,GAAqB,GAA9B,EAAmC,IAAnC,CAAX,CAA3G,CAR2C,CAS3C;;AACA,SAAKK,KAAL,GAAa,OAAOP,GAAG,CAACO,KAAX,KAAqB,WAArB,GAAmCP,GAAG,CAACO,KAAvC,GAA+C,GAA5D,CAV2C,CAY3C;;AACA,SAAKC,oBAAL,GAA4B,OAAOR,GAAG,CAACQ,oBAAX,KAAoC,WAApC,GAAkDR,GAAG,CAACQ,oBAAtD,GAA6E,MAAzG,CAb2C,CAc3C;;AACA,SAAKC,qBAAL,GAA6B,OAAOT,GAAG,CAACS,qBAAX,KAAqC,WAArC,GAAmDT,GAAG,CAACS,qBAAvD,GAA+E,IAA5G,CAf2C,CAgB3C;;AACA,SAAKC,WAAL,GAAmB,OAAOV,GAAG,CAACU,WAAX,KAA2B,WAA3B,GAAyCV,GAAG,CAACU,WAA7C,GAA2D,IAA9E,CAjB2C,CAkB3C;;AACA,SAAKC,iBAAL,GAAyB,OAAOX,GAAG,CAACW,iBAAX,KAAiC,WAAjC,GAA+CX,GAAG,CAACW,iBAAnD,GAAuE,IAAhG;AAEA,SAAKC,GAAL,GAAWC,KAAK,CAACC,KAAN,GAAcC,OAAd,CAAsB;AAC/BC,MAAAA,IAAI,EAAE,kBADyB;AAE/BC,MAAAA,GAAG,EAAE;AAF0B,KAAtB,CAAX;AAIA,SAAKC,QAAL,GAAgB,KAAKN,GAAL,CAASM,QAAzB,CAzB2C,CAyBR;AAEnC;AACA;;AACA,QAAG,OAAOlB,GAAG,CAACmB,0BAAX,KAA0C,WAA7C,EAA0D;AACxD;AACA,WAAKA,0BAAL,GAAkCnB,GAAG,CAACmB,0BAAtC;;AACA,UAAG,KAAKA,0BAAL,CAAgCC,MAAhC,KAA2CrB,WAA9C,EAA2D;AACzDsB,QAAAA,OAAO,CAACC,GAAR,CAAY,2EAAZ;AACD;;AACD,UAAIC,CAAC,GAAG,KAAKJ,0BAAb;AACA,UAAIK,CAAC,GAAG,GAAR;;AAAa,WAAI,IAAIC,CAAC,GAAC,CAAV,EAAYA,CAAC,GAACF,CAAC,CAACH,MAAhB,EAAuBK,CAAC,EAAxB,EAA4B;AAAED,QAAAA,CAAC,IAAGD,CAAC,CAACE,CAAD,CAAL;AAAW;;AACtD,UAAGrB,IAAI,CAACsB,GAAL,CAASF,CAAC,GAAC,GAAX,IAAgB,MAAnB,EAA2B;AAAEH,QAAAA,OAAO,CAACC,GAAR,CAAY,sDAAZ;AAAsE;AACpG,KATD,MASO;AACL,WAAKH,0BAAL,GAAkC,EAAlC;AACD,KAxC0C,CA0C3C;AACA;AACA;AACA;;;AACA,SAAKQ,UAAL,GAAkB7B,UAAU,GAAG,KAAKG,eAAlB,GAAoCF,WAAW,GAAG,KAAKE,eAAvD,GAAyEH,UAA3F;AACA,SAAKA,UAAL,GAAkBA,UAAlB;AACA,SAAKC,WAAL,GAAmBA,WAAnB;AACA,SAAK6B,WAAL,GAAmBxB,IAAI,CAACyB,GAAL,CAAS,KAAK5B,eAAd,EAA+B,CAA/B,CAAnB,CAjD2C,CAiDW;;AACtD,SAAK6B,YAAL,GAAoB,IAAIC,KAAJ,CAAU,KAAKH,WAAf,CAApB;AACA,SAAKI,aAAL,GAAqB,IAAID,KAAJ,CAAU,KAAKH,WAAf,CAArB;AACA,SAAKK,aAAL,GAAqB,IAAIF,KAAJ,CAAU,KAAKH,WAAf,CAArB;AACA,SAAKM,UAAL,GAAkB,IAAIH,KAAJ,CAAU,KAAKH,WAAf,CAAlB,CArD2C,CAuD3C;;AACA,QAAIO,UAAU,GAAG,EAAjB;AACA,SAAKC,EAAL,GAAU,IAAIC,EAAE,CAACC,UAAP,EAAV;AACA,SAAKF,EAAL,CAAQG,GAAR,CAAYF,EAAE,CAACG,MAAH,CAAUC,KAAV,CAAgB;AAACC,MAAAA,UAAU,EAAE,CAAC,EAAD,CAAb;AAAmBC,MAAAA,KAAK,EAAC,EAAzB;AAA6BC,MAAAA,UAAU,EAAE;AAAzC,KAAhB,CAAZ;AACA,SAAKR,EAAL,CAAQG,GAAR,CAAYF,EAAE,CAACG,MAAH,CAAUC,KAAV,CAAgB;AAACE,MAAAA,KAAK,EAAC,EAAP;AAAWC,MAAAA,UAAU,EAAE;AAAvB,KAAhB,CAAZ;AACA,SAAKR,EAAL,CAAQG,GAAR,CAAYF,EAAE,CAACG,MAAH,CAAUC,KAAV,CAAgB;AAACE,MAAAA,KAAK,EAAC,EAAP;AAAWC,MAAAA,UAAU,EAAE;AAAvB,KAAhB,CAAZ;AACA,SAAKR,EAAL,CAAQG,GAAR,CAAYF,EAAE,CAACG,MAAH,CAAUC,KAAV,CAAgB;AAC1BE,MAAAA,KAAK,EAAE,CADmB;AAE1BE,MAAAA,iBAAiB,EAAE,iBAFO;AAG1BC,MAAAA,iBAAiB,EAAE,MAHO;AAI1B9B,MAAAA,IAAI,EAAE;AAJoB,KAAhB,CAAZ,EA7D2C,CAmE3C;;AACA,SAAK+B,UAAL,GAAkB,EAAlB;AACA,SAAKC,SAAL,GAAiBX,EAAE,CAACY,KAAH,CAASC,GAAT,CAAa,KAAb,CAAjB,CArE2C,CAuE3C;;AACA,SAAKC,UAAL,GAAkB,EAAlB,CAxE2C,CA0E3C;;AACA,SAAKC,GAAL,GAAW,CAAX,CA3E2C,CA2E7B;;AACd,SAAKC,cAAL,GAAsB,CAAtB,CA5E2C,CA4ElB;;AACzB,SAAKC,OAAL,GAAe,GAAf,CA7E2C,CA6EvB;;AACpB,SAAKC,aAAL,GAAqB,CAArB;AACA,SAAKC,gBAAL,GAAwB,EAAxB;AACA,SAAKC,qBAAL,GAA6B,IAAIC,OAAO,CAACC,MAAZ,CAAmB,IAAnB,EAAyB,EAAzB,CAA7B;AACA,SAAKC,mBAAL,GAA2B,IAAIF,OAAO,CAACC,MAAZ,CAAmB,IAAnB,EAAyB,EAAzB,CAA3B;AACA,SAAKE,QAAL,GAAgB,IAAhB;AACD;;AAEDC,EAAAA,aAAa,GAAG;AACd;AACA;AACA;AACA;AACA,QAAG,KAAK3C,0BAAL,CAAgCC,MAAhC,KAA2C,CAA9C,EAAiD;AAC/C,aAAO2C,SAAS,CAACC,KAAV,CAAgB,CAAhB,EAAmB,KAAKjE,WAAxB,CAAP;AACD,KAFD,MAEO;AACL;AACA,UAAIkE,CAAC,GAAGF,SAAS,CAACG,KAAV,CAAgB,CAAhB,EAAmB,GAAnB,CAAR;AACA,UAAIC,OAAO,GAAG,GAAd;;AACA,WAAI,IAAI1C,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAK1B,WAAnB,EAA+B0B,CAAC,EAAhC,EAAoC;AAClC0C,QAAAA,OAAO,IAAI,KAAKhD,0BAAL,CAAgCM,CAAhC,CAAX;;AACA,YAAGwC,CAAC,GAAGE,OAAP,EAAgB;AAAE,iBAAO1C,CAAP;AAAW;AAC9B;AACF;AACF;;AACD2C,EAAAA,MAAM,CAAC5C,CAAD,EAAI;AACR;AACA;AACA,QAAI6C,IAAI,GAAG,IAAIN,SAAS,CAACO,GAAd,CAAkB,CAAlB,EAAqB,CAArB,EAAwB,KAAK3C,UAA7B,CAAX;AACA0C,IAAAA,IAAI,CAACE,CAAL,GAAS/C,CAAT;AACA,QAAIgD,IAAI,GAAGnC,EAAE,CAACoC,MAAH,CAAUjD,CAAV,CAAX;AACAgD,IAAAA,IAAI,GAAGA,IAAI,CAACE,OAAL,CAAa,CAAC,CAAD,EAAI,EAAJ,CAAb,CAAP;AACA,QAAIC,aAAa,GAAG,KAAKvC,EAAL,CAAQwC,KAAR,CAAcJ,IAAd,CAApB;AACA,QAAIK,GAAG,GAAG;AAACC,MAAAA,MAAM,EAAEH,aAAa,CAAC9C,GAAd,GAAoBkD,QAApB,GAA+B,CAA/B,CAAT;AAA4CC,MAAAA,KAAK,EAAEL,aAAa,CAACM,MAAd,CAAqB,CAArB,EAAwBF,QAAxB,GAAmC,CAAnC;AAAnD,KAAV;AACAJ,IAAAA,aAAa,CAACO,OAAd;AACAV,IAAAA,IAAI,CAACU,OAAL;AACA,WAAOL,GAAP;AACD;;AACDM,EAAAA,WAAW,CAACC,EAAD,EAAK;AACd;AACA;AACA,QAAIb,CAAC,GAAG,EAAR;AACAA,IAAAA,CAAC,GAAGA,CAAC,CAACc,MAAF,CAASD,EAAT,CAAJ,CAJc,CAII;AAClB;;AACA,QAAIE,CAAC,GAAG,KAAK1D,WAAb;;AACA,SAAI,IAAIH,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKxB,eAAnB,EAAmCwB,CAAC,EAApC,EAAwC;AACtC;AACA8C,MAAAA,CAAC,GAAGA,CAAC,CAACc,MAAF,CAAS,KAAKvD,YAAL,CAAkBwD,CAAC,GAAC,CAAF,GAAI7D,CAAtB,CAAT,CAAJ,CAFsC,CAGtC;AACA;;AACA,UAAI8D,UAAU,GAAG,IAAIxD,KAAJ,CAAU,KAAKhC,WAAf,CAAjB;;AACA,WAAI,IAAIyF,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAC,KAAKzF,WAAnB,EAA+ByF,CAAC,EAAhC,EAAoCD,UAAU,CAACC,CAAD,CAAV,GAAgB,GAAhB;;AACpCD,MAAAA,UAAU,CAAC,KAAKvD,aAAL,CAAmBsD,CAAC,GAAC,CAAF,GAAI7D,CAAvB,CAAD,CAAV,GAAwC,MAAI,KAAK3B,UAAjD;AACAyE,MAAAA,CAAC,GAAGA,CAAC,CAACc,MAAF,CAASE,UAAT,CAAJ;AACD;;AACD,WAAOhB,CAAP;AACD;;AACDkB,EAAAA,OAAO,CAACC,WAAD,EAAc;AACnB;AACA,SAAKrC,cAAL,IAAuB,CAAvB;AACA,SAAKG,gBAAL,GAAwBkC,WAAxB,CAHmB,CAGkB;AAErC;;AACA,QAAIZ,MAAJ;;AACA,QAAG,KAAKzB,cAAL,GAAsB,KAAKpD,eAA9B,EAA+C;AAC7C;AACA,UAAI0F,SAAS,GAAG,KAAKR,WAAL,CAAiBO,WAAjB,CAAhB;;AACA,UAAG,KAAK7B,QAAR,EAAkB;AAChB;AACA,aAAKP,OAAL,GAAelD,IAAI,CAACE,GAAL,CAAS,GAAT,EAAcF,IAAI,CAACyB,GAAL,CAAS,KAAKnB,WAAd,EAA2B,MAAI,CAAC,KAAK0C,GAAL,GAAW,KAAK3C,qBAAjB,KAAyC,KAAKD,oBAAL,GAA4B,KAAKC,qBAA1E,CAA/B,CAAd,CAAf;AACD,OAHD,MAGO;AACL,aAAK6C,OAAL,GAAe,KAAK3C,iBAApB,CADK,CACkC;AACxC;;AACD,UAAIiF,EAAE,GAAGxF,IAAI,CAACyF,MAAL,EAAT;;AACA,UAAGD,EAAE,GAAG,KAAKtC,OAAb,EAAsB;AACpB;AACAwB,QAAAA,MAAM,GAAG,KAAKhB,aAAL,EAAT;AACD,OAHD,MAGO;AACL;AACA,YAAIgC,MAAM,GAAG,KAAK1B,MAAL,CAAYuB,SAAZ,CAAb;AACAb,QAAAA,MAAM,GAAGgB,MAAM,CAACd,KAAhB;AACF;AACD,KAlBD,MAkBO;AACL;AACA;AACA,UAAIW,SAAS,GAAG,EAAhB;AACAb,MAAAA,MAAM,GAAG,KAAKhB,aAAL,EAAT;AACD,KA9BkB,CAgCnB;;;AACA,SAAK5B,UAAL,CAAgB6D,KAAhB;AACA,SAAK7D,UAAL,CAAgB8D,IAAhB,CAAqBL,SAArB;AACA,SAAK7D,YAAL,CAAkBiE,KAAlB;AACA,SAAKjE,YAAL,CAAkBkE,IAAlB,CAAuBN,WAAvB;AACA,SAAK1D,aAAL,CAAmB+D,KAAnB;AACA,SAAK/D,aAAL,CAAmBgE,IAAnB,CAAwBlB,MAAxB;AAEA,WAAOA,MAAP;AACD;;AAEDmB,EAAAA,gBAAgB,CAACC,SAAD,EAAW,CAE1B;AAED;;;;;;;;;;AAQAC,EAAAA,kBAAkB,CAACD,SAAD,EAAY3F,KAAZ,EAAmByC,SAAnB,EAA8B;AAC9C;AACA,UAAMoD,KAAK,GAAG,KAAKC,YAAL,CAAkBC,MAAlB,CAAyBJ,SAAzB,CAAd;;AACA,UAAMK,YAAY,GAAG,MAAMlE,EAAE,CAACmE,IAAH,CAAQ,MAAM;AACvC,YAAMC,WAAW,GAAGC,cAAc,CAC9BN,KAAK,CAACO,GAAN,CAAUC,OAAO,IAAIA,OAAO,CAAC,CAAD,CAA5B,CAD8B,EACI,KAAKC,IAAL,CAAUC,MADd,EACsB,KAAKD,IAAL,CAAUE,KADhC,CAAlC;AAEA,YAAMC,YAAY,GAAG3E,EAAE,CAAC4E,QAAH,CACjBb,KAAK,CAACO,GAAN,CAAUC,OAAO,IAAIA,OAAO,CAAC,CAAD,CAA5B,CADiB,EACiB,OADjB,CAArB;AAEA,YAAMM,EAAE,GAAG,KAAKC,aAAL,CAAmBvC,KAAnB,CAAyB6B,WAAzB,EAAsC;AAACW,QAAAA,QAAQ,EAAE;AAAX,OAAtC,EACNC,GADM,CACFhF,EAAE,CAACiF,MAAH,CAAUN,YAAV,EAAwBO,WAAxB,CADE,EACoCC,GADpC,CACwC,CAAC,CADzC,CAAX;AAGA,YAAMC,YAAY,GAAGpF,EAAE,CAAC4E,QAAH,CAAYb,KAAK,CAACO,GAAN,CAAUC,OAAO,IAAIA,OAAO,CAAC,CAAD,CAA5B,CAAZ,CAArB;AACA,YAAMc,eAAe,GAAGhB,cAAc,CAClCN,KAAK,CAACO,GAAN,CAAUC,OAAO,IAAIA,OAAO,CAAC,CAAD,CAA5B,CADkC,EACA,KAAKC,IAAL,CAAUC,MADV,EACkB,KAAKD,IAAL,CAAUE,KAD5B,CAAtC;AAEA,YAAMY,cAAc,GAChB,KAAKC,aAAL,CAAmBC,OAAnB,CAA2BH,eAA3B,EAA4C7F,GAA5C,CAAgD,CAAC,CAAjD,CADJ;AAEA,YAAMiG,QAAQ,GAAGzF,EAAE,CAAC0F,MAAH,CAAU,CAAV,EAAaC,GAAb,CACb3F,EAAE,CAAC4E,QAAH,CAAYb,KAAK,CAACO,GAAN,CAAUC,OAAO,IAAIA,OAAO,CAAC,CAAD,CAA5B,CAAZ,EAA8CqB,MAA9C,CAAqD,SAArD,CADa,CAAjB;AAEA,YAAMC,QAAQ,GACVT,YAAY,CAAClF,GAAb,CAAiBoF,cAAc,CAACN,GAAf,CAAmBS,QAAnB,EAA6BT,GAA7B,CAAiC9G,KAAjC,CAAjB,CADJ;AAEA,aAAO8B,EAAE,CAAC8F,MAAH,CAAUC,gBAAV,CAA2BF,QAA3B,EAAqChB,EAArC,CAAP;AACD,KAlB0B,CAA3B,CAH8C,CAuB9C;AACA;;;AACA,UAAMmB,KAAK,GAAGhG,EAAE,CAACiG,aAAH,CAAiB/B,YAAjB,CAAd,CAzB8C,CA0B9C;;AACAvD,IAAAA,SAAS,CAACuF,cAAV,CAAyBF,KAAK,CAACA,KAA/B;AACAhG,IAAAA,EAAE,CAAC6C,OAAH,CAAWmD,KAAX,EA5B8C,CA6B9C;AACD;;AAEDG,EAAAA,QAAQ,CAACC,MAAD,EAAS;AACf,SAAKlF,aAAL,GAAqBkF,MAArB;AACA,SAAKhF,qBAAL,CAA2BlB,GAA3B,CAA+BkG,MAA/B;AACA,SAAKxG,aAAL,CAAmB8D,KAAnB;AACA,SAAK9D,aAAL,CAAmB+D,IAAnB,CAAwByC,MAAxB;;AAEA,QAAG,CAAC,KAAK5E,QAAT,EAAmB;AAAE;AAAS,KANf,CAQf;;;AACA,SAAKT,GAAL,IAAY,CAAZ,CATe,CAWf;AACA;;AACA,QAAG,KAAKC,cAAL,GAAsB,KAAKpD,eAAL,GAAuB,CAAhD,EAAmD;AACjD,UAAIyI,CAAC,GAAG,IAAInJ,UAAJ,EAAR;AACA,UAAI+F,CAAC,GAAG,KAAK1D,WAAb;AACA8G,MAAAA,CAAC,CAACjJ,MAAF,GAAW,KAAKyC,UAAL,CAAgBoD,CAAC,GAAC,CAAlB,CAAX;AACAoD,MAAAA,CAAC,CAAChJ,OAAF,GAAY,KAAKsC,aAAL,CAAmBsD,CAAC,GAAC,CAArB,CAAZ;AACAoD,MAAAA,CAAC,CAAC/I,OAAF,GAAY,KAAKsC,aAAL,CAAmBqD,CAAC,GAAC,CAArB,CAAZ;AACAoD,MAAAA,CAAC,CAAC9I,MAAF,GAAW,KAAKsC,UAAL,CAAgBoD,CAAC,GAAC,CAAlB,CAAX;;AACA,UAAG,KAAKnC,UAAL,CAAgB/B,MAAhB,GAAyB,KAAKlB,eAAjC,EAAkD;AAChD,aAAKiD,UAAL,CAAgB6C,IAAhB,CAAqB0C,CAArB;AACD,OAFD,MAEO;AACL;AACA,YAAIC,EAAE,GAAG5E,SAAS,CAACC,KAAV,CAAgB,CAAhB,EAAmB,KAAK9D,eAAxB,CAAT;AACA,aAAKiD,UAAL,CAAgBwF,EAAhB,IAAsBD,CAAtB;AACD;AACF;;AACD,UAAMnC,YAAY,GAAG,MAAMlE,EAAE,CAACmE,IAAH,CAAQ,MAAI;AACrC,UAAIoC,QAAQ,GAAGvG,EAAE,CAACoC,MAAH,CAAUoE,CAAC,CAACtE,CAAZ,EAAe,CAAC,CAAD,EAAI,EAAJ,CAAf,CAAf;AACA,UAAIuE,QAAQ,GAAGzG,EAAE,CAACoC,MAAH,CAAUsE,CAAV,CAAf;AACA,UAAIC,GAAG,GAAG3G,EAAE,CAACoC,MAAH,CAAUwE,KAAV,CAAV;AACA,UAAIC,MAAM,GAAG,KAAK9G,EAAL,CAAQwC,KAAR,CAAcgE,QAAd,CAAb;AACAM,MAAAA,MAAM,GAAGA,MAAM,CAACxE,OAAP,CAAe,CAACwE,MAAM,CAACC,KAAP,CAAa,CAAb,CAAD,CAAf,CAAT;AACA,YAAMC,IAAI,GAAG/G,EAAE,CAACgH,SAAH,CAAaH,MAAb,EAAqBF,GAArB,EAA0BhB,GAA1B,CAA8Bc,QAA9B,EAAwCQ,MAAxC,GAAiD9B,GAAjD,GAAuDH,GAAvD,CAA2D,GAA3D,CAAb;AACA,aAAO+B,IAAP;AACD,KAR0B,CAA3B,CA5Be,CAqCf;AACA;;;AACA,QAAG,KAAKjG,UAAL,CAAgB/B,MAAhB,GAAyB,KAAKjB,qBAAjC,EAAwD;AACtD,UAAIoJ,MAAM,GAAG,GAAb;;AACA,WAAI,IAAI9H,CAAC,GAAC,CAAV,EAAYA,CAAC,GAAG,KAAKsB,UAArB,EAAgCtB,CAAC,EAAjC,EAAqC;AACnC,YAAI+H,EAAE,GAAGzF,SAAS,CAACC,KAAV,CAAgB,CAAhB,EAAmB,KAAKb,UAAL,CAAgB/B,MAAnC,CAAT;AACA,YAAIsH,CAAC,GAAG,KAAKvF,UAAL,CAAgBqG,EAAhB,CAAR;AACA,YAAIX,CAAC,GAAG,IAAI9E,SAAS,CAACO,GAAd,CAAkB,CAAlB,EAAqB,CAArB,EAAwB,KAAK3C,UAA7B,CAAR;AACAkH,QAAAA,CAAC,CAACtE,CAAF,GAAMmE,CAAC,CAACjJ,MAAR;AACA,YAAIqG,MAAM,GAAG,KAAK1B,MAAL,CAAYsE,CAAC,CAAC9I,MAAd,CAAb;AACA,YAAI6J,CAAC,GAAGf,CAAC,CAAC/I,OAAF,GAAY,KAAKY,KAAL,GAAauF,MAAM,CAACd,KAAxC;AACA,YAAI+D,CAAC,GAAG,CAAC,CAAD,EAAI,CAAJ,EAAO,CAAP,EAAU,CAAV,EAAY,CAAZ,CAAR;AACAA,QAAAA,CAAC,CAACL,CAAC,CAAChJ,OAAH,CAAD,GAAe+J,CAAf;AACA,YAAIR,KAAK,GAAG,CAAC,CAAD,EAAG,CAAH,EAAK,CAAL,EAAO,CAAP,EAAS,CAAT,CAAZ;AACAA,QAAAA,KAAK,CAACP,CAAC,CAAChJ,OAAH,CAAL,GAAmB,CAAnB;AAGA6J,QAAAA,MAAM,IAAIhD,YAAY,GAAGxB,QAAf,GAA0B,CAA1B,CAAV;AACD;;AACG,YAAMsD,KAAK,GAAGhG,EAAE,CAACiG,aAAH,CAAiB/B,YAAjB,EAA+B,KAAKnE,EAAL,CAAQsH,UAAR,EAA/B,CAAd;AACA,WAAK1G,SAAL,CAAeuF,cAAf,CAA8BF,KAAK,CAACA,KAApC;AACAxH,MAAAA,KAAK,CAAC8I,IAAN,CAAWC,KAAX,CAAiB;AAAC5I,QAAAA,IAAI,EAAE;AAAP,OAAjB,EAA0C,KAAKoB,EAAL,CAAQyH,QAAR,CAAiB7I,IAAI,GAAC,QAAtB,CAA1C;AACA,WAAKoB,EAAL,CAAQsH,UAAR,GAAqB,CAArB,EAAwBI,KAAxB;AACAzI,MAAAA,OAAO,CAACC,GAAR,CAAY,sBAAZ;AACJiI,MAAAA,MAAM,GAAGA,MAAM,GAAC,KAAKxG,UAArB;AACA,WAAKa,mBAAL,CAAyBrB,GAAzB,CAA6BgH,MAA7B;AACAlI,MAAAA,OAAO,CAACC,GAAR,CAAY,SAAZ,EAAuB,KAAKsC,mBAAL,CAAyBmG,WAAzB,EAAvB;AACD;AACF;;AACDC,EAAAA,OAAO,CAACC,GAAD,EAAM;AACXA,IAAAA,GAAG,CAACC,SAAJ,GAAgB,EAAhB,CADW,CACS;AAEpB;;AACA,QAAIC,QAAQ,GAAGC,QAAQ,CAACC,aAAT,CAAuB,KAAvB,CAAf,CAJW,CAMX;;AACA,QAAIC,IAAI,GAAGF,QAAQ,CAACC,aAAT,CAAuB,KAAvB,CAAX;AACA,QAAIE,CAAC,GAAG,EAAR;AACAA,IAAAA,CAAC,IAAI,6BAA6B,KAAKpH,UAAL,CAAgB/B,MAA7C,GAAsD,MAA3D;AACAmJ,IAAAA,CAAC,IAAI,0BAA0B,KAAKjH,OAA/B,GAAyC,MAA9C;AACAiH,IAAAA,CAAC,IAAI,UAAU,KAAKnH,GAAf,GAAqB,MAA1B;AACAmH,IAAAA,CAAC,IAAI,8BAA8B,KAAK3G,mBAAL,CAAyBmG,WAAzB,EAA9B,GAAuE,QAA5E;AACAQ,IAAAA,CAAC,IAAI,wBAAwB,KAAK9G,qBAAL,CAA2BsG,WAA3B,EAAxB,GAAmE,QAAxE;AACAO,IAAAA,IAAI,CAACJ,SAAL,GAAiBK,CAAjB;AACAJ,IAAAA,QAAQ,CAACK,WAAT,CAAqBF,IAArB;AAEAL,IAAAA,GAAG,CAACO,WAAJ,CAAgBL,QAAhB;AACD;;AAlTS","file":"tfjs-brain.map","sourceRoot":"..","sourcesContent":["  \"use strict\";\r\n  \r\n  // An agent is in state0 and does action0\r\n  // environment then assigns reward0 and provides new state, state1\r\n  // Experience nodes store all this information, which is used in the\r\n  // Q-learning update step\r\n  class Experience  {\r\n    constructor(state0, action0, reward0, state1){\r\n      this.state0 = state0;\r\n      this.action0 = action0;\r\n      this.reward0 = reward0;\r\n      this.state1 = state1;        \r\n    }\r\n  }\r\n\r\n  // A Brain object does all the magic.\r\n  // over time it receives some inputs and some rewards\r\n  // and its job is to set the outputs to maximize the expected reward\r\n  class Brain {\r\n    constructor(num_states, num_actions, opt={}) {\r\n      // in number of time steps, of temporal memory\r\n      // the ACTUAL input to the net will be (x,a) temporal_window times, and followed by current x\r\n      // so to have no information from previous time step going into value function, set to 0.\r\n      this.temporal_window = typeof opt.temporal_window !== 'undefined' ? opt.temporal_window : 1; \r\n      // size of experience replay memory\r\n      this.experience_size = typeof opt.experience_size !== 'undefined' ? opt.experience_size : 30000;\r\n      // number of examples in experience replay memory before we begin learning\r\n      this.start_learn_threshold = typeof opt.start_learn_threshold !== 'undefined'? opt.start_learn_threshold : Math.floor(Math.min(this.experience_size*0.1, 1000)); \r\n      // gamma is a crucial parameter that controls how much plan-ahead the agent does. In [0,1]\r\n      this.gamma = typeof opt.gamma !== 'undefined' ? opt.gamma : 0.8;\r\n      \r\n      // number of steps we will learn for\r\n      this.learning_steps_total = typeof opt.learning_steps_total !== 'undefined' ? opt.learning_steps_total : 100000;\r\n      // how many steps of the above to perform only random actions (in the beginning)?\r\n      this.learning_steps_burnin = typeof opt.learning_steps_burnin !== 'undefined' ? opt.learning_steps_burnin : 3000;\r\n      // what epsilon value do we bottom out on? 0.0 => purely deterministic policy at end\r\n      this.epsilon_min = typeof opt.epsilon_min !== 'undefined' ? opt.epsilon_min : 0.05;\r\n      // what epsilon to use at test time? (i.e. when learning is disabled)\r\n      this.epsilon_test_time = typeof opt.epsilon_test_time !== 'undefined' ? opt.epsilon_test_time : 0.01;\r\n      \r\n      this.vis = tfvis.visor().surface({\r\n        name: 'My First Surface',\r\n        tab: 'Input Data'\r\n      });\r\n      this.drawArea = this.vis.drawArea; // Get the examples\r\n\r\n      // advanced feature. Sometimes a random action should be biased towards some values\r\n      // for example in flappy bird, we may want to choose to not flap more often\r\n      if(typeof opt.random_action_distribution !== 'undefined') {\r\n        // this better sum to 1 by the way, and be of length this.num_actions\r\n        this.random_action_distribution = opt.random_action_distribution;\r\n        if(this.random_action_distribution.length !== num_actions) {\r\n          console.log('TROUBLE. random_action_distribution should be same length as num_actions.');\r\n        }\r\n        var a = this.random_action_distribution;\r\n        var s = 0.0; for(var k=0;k<a.length;k++) { s+= a[k]; }\r\n        if(Math.abs(s-1.0)>0.0001) { console.log('TROUBLE. random_action_distribution should sum to 1!'); }\r\n      } else {\r\n        this.random_action_distribution = [];\r\n      }\r\n      \r\n      // states that go into neural net to predict optimal action look as\r\n      // x0,a0,x1,a1,x2,a2,...xt\r\n      // this variable controls the size of that temporal window. Actions are\r\n      // encoded as 1-of-k hot vectors\r\n      this.net_inputs = num_states * this.temporal_window + num_actions * this.temporal_window + num_states;\r\n      this.num_states = num_states;\r\n      this.num_actions = num_actions;\r\n      this.window_size = Math.max(this.temporal_window, 2); // must be at least 2, but if we want more context even more\r\n      this.state_window = new Array(this.window_size);\r\n      this.action_window = new Array(this.window_size);\r\n      this.reward_window = new Array(this.window_size);\r\n      this.net_window = new Array(this.window_size);\r\n      \r\n      // create [state -> value of all possible actions] modeling net for the value function\r\n      var layer_defs = [];\r\n      this.NN = new tf.sequential();\r\n      this.NN.add(tf.layers.dense({inputShape: [65], units:50, activation: 'relu'}));\r\n      this.NN.add(tf.layers.dense({units:50, activation: 'relu'}));\r\n      this.NN.add(tf.layers.dense({units:50, activation: 'relu'}));\r\n      this.NN.add(tf.layers.dense({\r\n        units: 5,\r\n        kernelInitializer: 'varianceScaling',\r\n        kernelRegularizer: 'l1l2',\r\n        name: \"outter\"\r\n      }));\r\n      //this.NN.compile({loss: 'meanSquaredError', optimizer: 'sgd'});\r\n      this.BATCH_SIZE = 64;\r\n      this.optimizer = tf.train.sgd(0.001);\r\n      \r\n      // experience replay\r\n      this.experience = [];\r\n      \r\n      // various housekeeping variables\r\n      this.age = 0; // incremented every backward()\r\n      this.forward_passes = 0; // incremented every forward()\r\n      this.epsilon = 1.0; // controls exploration exploitation tradeoff. Should be annealed over time\r\n      this.latest_reward = 0;\r\n      this.last_input_array = [];\r\n      this.average_reward_window = new cnnutil.Window(1000, 10);\r\n      this.average_loss_window = new cnnutil.Window(1000, 10);\r\n      this.learning = true;\r\n    }\r\n\r\n    random_action() {\r\n      // a bit of a helper function. It returns a random action\r\n      // we are abstracting this away because in future we may want to \r\n      // do more sophisticated things. For example some actions could be more\r\n      // or less likely at \"rest\"/default state.\r\n      if(this.random_action_distribution.length === 0) {\r\n        return convnetjs.randi(0, this.num_actions);\r\n      } else {\r\n        // okay, lets do some fancier sampling:\r\n        var p = convnetjs.randf(0, 1.0);\r\n        var cumprob = 0.0;\r\n        for(var k=0;k<this.num_actions;k++) {\r\n          cumprob += this.random_action_distribution[k];\r\n          if(p < cumprob) { return k; }\r\n        }\r\n      }\r\n    }\r\n    policy(s) {\r\n      // compute the value of doing any action in this state\r\n      // and return the argmax action and its value\r\n      var svol = new convnetjs.Vol(1, 1, this.net_inputs);\r\n      svol.w = s;\r\n      let tens = tf.tensor(s);\r\n      tens = tens.reshape([1, 65]);\r\n      var action_values = this.NN.apply(tens);\r\n      let ret = {action: action_values.max().dataSync()[0], value: action_values.argMax(1).dataSync()[0]};\r\n      action_values.dispose();\r\n      tens.dispose();\r\n      return ret;\r\n    }\r\n    getNetInput(xt) {\r\n      // return s = (x,a,x,a,x,a,xt) state vector. \r\n      // It's a concatenation of last window_size (x,a) pairs and current state x\r\n      var w = [];\r\n      w = w.concat(xt); // start with current state\r\n      // and now go backwards and append states and actions from history temporal_window times\r\n      var n = this.window_size; \r\n      for(var k=0;k<this.temporal_window;k++) {\r\n        // state\r\n        w = w.concat(this.state_window[n-1-k]);\r\n        // action, encoded as 1-of-k indicator vector. We scale it up a bit because\r\n        // we dont want weight regularization to undervalue this information, as it only exists once\r\n        var action1ofk = new Array(this.num_actions);\r\n        for(var q=0;q<this.num_actions;q++) action1ofk[q] = 0.0;\r\n        action1ofk[this.action_window[n-1-k]] = 1.0*this.num_states;\r\n        w = w.concat(action1ofk);\r\n      }\r\n      return w;\r\n    }\r\n    forward(input_array) {\r\n      // compute forward (behavior) pass given the input neuron signals from body\r\n      this.forward_passes += 1;\r\n      this.last_input_array = input_array; // back this up\r\n      \r\n      // create network input\r\n      var action;\r\n      if(this.forward_passes > this.temporal_window) {\r\n        // we have enough to actually do something reasonable\r\n        var net_input = this.getNetInput(input_array);\r\n        if(this.learning) {\r\n          // compute epsilon for the epsilon-greedy policy\r\n          this.epsilon = Math.min(1.0, Math.max(this.epsilon_min, 1.0-(this.age - this.learning_steps_burnin)/(this.learning_steps_total - this.learning_steps_burnin))); \r\n        } else {\r\n          this.epsilon = this.epsilon_test_time; // use test-time value\r\n        }\r\n        var rf = Math.random();\r\n        if(rf < this.epsilon) {\r\n          // choose a random action with epsilon probability\r\n          action = this.random_action();\r\n        } else {\r\n          // otherwise use our policy to make decision\r\n          var maxact = this.policy(net_input);\r\n          action = maxact.value;\r\n       }\r\n      } else {\r\n        // pathological case that happens first few iterations \r\n        // before we accumulate window_size inputs\r\n        var net_input = [];\r\n        action = this.random_action();\r\n      }\r\n      \r\n      // remember the state and action we took for backward pass\r\n      this.net_window.shift();\r\n      this.net_window.push(net_input);\r\n      this.state_window.shift(); \r\n      this.state_window.push(input_array);\r\n      this.action_window.shift(); \r\n      this.action_window.push(action);\r\n      \r\n      return action;\r\n    }\r\n\r\n    sampleExperience(batchSize){\r\n      \r\n    }\r\n\r\n    /**\r\n     * Perform training on a randomly sampled batch from the replay buffer.\r\n     *\r\n     * @param {number} batchSize Batch size.\r\n     * @param {number} gamma Reward discount rate. Must be >= 0 and <= 1.\r\n     * @param {tf.train.Optimizer} optimizer The optimizer object used to update\r\n     *   the weights of the online network.\r\n     */\r\n    trainOnReplayBatch(batchSize, gamma, optimizer) {\r\n      // Get a batch of examples from the replay buffer.\r\n      const batch = this.replayMemory.sample(batchSize);\r\n      const lossFunction = () => tf.tidy(() => {\r\n        const stateTensor = getStateTensor(\r\n            batch.map(example => example[0]), this.game.height, this.game.width);\r\n        const actionTensor = tf.tensor1d(\r\n            batch.map(example => example[1]), 'int32');\r\n        const qs = this.onlineNetwork.apply(stateTensor, {training: true})\r\n            .mul(tf.oneHot(actionTensor, NUM_ACTIONS)).sum(-1);\r\n\r\n        const rewardTensor = tf.tensor1d(batch.map(example => example[2]));\r\n        const nextStateTensor = getStateTensor(\r\n            batch.map(example => example[4]), this.game.height, this.game.width);\r\n        const nextMaxQTensor =\r\n            this.targetNetwork.predict(nextStateTensor).max(-1);\r\n        const doneMask = tf.scalar(1).sub(\r\n            tf.tensor1d(batch.map(example => example[3])).asType('float32'));\r\n        const targetQs =\r\n            rewardTensor.add(nextMaxQTensor.mul(doneMask).mul(gamma));\r\n        return tf.losses.meanSquaredError(targetQs, qs);\r\n      });\r\n\r\n      // Calculate the gradients of the loss function with repsect to the weights\r\n      // of the online DQN.\r\n      const grads = tf.variableGrads(lossFunction);\r\n      // Use the gradients to update the online DQN's weights.\r\n      optimizer.applyGradients(grads.grads);\r\n      tf.dispose(grads);\r\n      // TODO(cais): Return the loss value here?\r\n    }\r\n\r\n    backward(reward) {\r\n      this.latest_reward = reward;\r\n      this.average_reward_window.add(reward);\r\n      this.reward_window.shift();\r\n      this.reward_window.push(reward);\r\n      \r\n      if(!this.learning) { return; } \r\n      \r\n      // various book-keeping\r\n      this.age += 1;\r\n      \r\n      // it is time t+1 and we have to store (s_t, a_t, r_t, s_{t+1}) as new experience\r\n      // (given that an appropriate number of state measurements already exist, of course)\r\n      if(this.forward_passes > this.temporal_window + 1) {\r\n        var e = new Experience();\r\n        var n = this.window_size;\r\n        e.state0 = this.net_window[n-2];\r\n        e.action0 = this.action_window[n-2];\r\n        e.reward0 = this.reward_window[n-2];\r\n        e.state1 = this.net_window[n-1];\r\n        if(this.experience.length < this.experience_size) {\r\n          this.experience.push(e);\r\n        } else {\r\n          // replace. finite memory!\r\n          var ri = convnetjs.randi(0, this.experience_size);\r\n          this.experience[ri] = e;\r\n        }\r\n      }\r\n      const lossFunction = () => tf.tidy(()=>{\r\n        let x_tensor = tf.tensor(x.w, [1, 65]);\r\n        let y_tensor = tf.tensor(y);\r\n        let y_s = tf.tensor(y_new);\r\n        let output = this.NN.apply(x_tensor);\r\n        output = output.reshape([output.shape[1]]);\r\n        const loss = tf.mulStrict(output, y_s).sub(y_tensor).square().sum().mul(0.5);\r\n        return loss;\r\n      });\r\n      // learn based on experience, once we have some samples to go on\r\n      // this is where the magic happens...\r\n      if(this.experience.length > this.start_learn_threshold) {\r\n        var avcost = 0.0;\r\n        for(var k=0;k < this.BATCH_SIZE;k++) {\r\n          var re = convnetjs.randi(0, this.experience.length);\r\n          var e = this.experience[re];\r\n          var x = new convnetjs.Vol(1, 1, this.net_inputs);\r\n          x.w = e.state0;\r\n          var maxact = this.policy(e.state1);\r\n          var r = e.reward0 + this.gamma * maxact.value;\r\n          var y = [0, 0, 0, 0,0];\r\n          y[e.action0] = r;\r\n          var y_new = [0,0,0,0,0]; \r\n          y_new[e.action0] = 1;\r\n\r\n\r\n          avcost += lossFunction().dataSync()[0];\r\n        }\r\n            const grads = tf.variableGrads(lossFunction, this.NN.getWeights());\r\n            this.optimizer.applyGradients(grads.grads);\r\n            tfvis.show.layer({name: 'Model Summary'}, this.NN.getLayer(name='outter'));\r\n            this.NN.getWeights()[2].print();\r\n            console.log(\"OPTIMIZER DOING THIS\");\r\n        avcost = avcost/this.BATCH_SIZE;\r\n        this.average_loss_window.add(avcost);\r\n        console.log(\"avg: %s\", this.average_loss_window.get_average())\r\n      }\r\n    }\r\n    visSelf(elt) {\r\n      elt.innerHTML = ''; // erase elt first\r\n      \r\n      // elt is a DOM element that this function fills with brain-related information\r\n      var brainvis = document.createElement('div');\r\n      \r\n      // basic information\r\n      var desc = document.createElement('div');\r\n      var t = '';\r\n      t += 'experience replay size: ' + this.experience.length + '<br>';\r\n      t += 'exploration epsilon: ' + this.epsilon + '<br>';\r\n      t += 'age: ' + this.age + '<br>';\r\n      t += 'average Q-learning loss: ' + this.average_loss_window.get_average() + '<br />';\r\n      t += 'smooth-ish reward: ' + this.average_reward_window.get_average() + '<br />';\r\n      desc.innerHTML = t;\r\n      brainvis.appendChild(desc);\r\n      \r\n      elt.appendChild(brainvis);\r\n    }\r\n  }\r\n  "]}